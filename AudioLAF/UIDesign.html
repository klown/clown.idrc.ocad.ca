<!DOCTYPE HTML PUBLIC "-//SoftQuad//DTD draft HTML 3.2 + extensions for HoTMetaL PRO 3.0 19960802//EN" "hmpro3.dtd"><HTML><HEAD><TITLE>Designing an Audio Look and Keyboard Feel For Swing</TITLE></HEAD><BODY><H1>Designing an Audio Look and Keyboard Feel For Swing</H1><H2>Swing and Its Pluggable Look and Feel</H2><P>Sun's &quot;Swing set&quot; is a set of fully Java implemented userinterface elements or components.  By &quot;fully Java implemented&quot; ismeant that all of the code that draws them on a user's screen, and that reactsto the user's input, is written in Java.  In contrast, the AWT components relyon OS specific peers for drawing and some event handling.  Another way ofstating this is, Swing components do not delegate their look and feel to a peer.</P><P>In addition, Swing is designed to delegate a component's look and feel to auser interface that is separate from the component itself.  In the abstract, aSwing component is made up of two parts; its user interface (UI) and an internalmodel that maintains the current state of the component.  The UI defines how thecomponent is presented, and allows users to interact with the component, whilethe model is a representation of what that component &quot;really is&quot;.  Thecomponent's model is updated according to user interactions, which, in turn,result in changes in the way the component is displayed.  For example, a pushbutton's model will include states such as &quot;pressed&quot; and &quot;disabled&quot;. Visually, the UI will draw the button on screen in an &quot;up&quot; position,if the button is not pressed, and (typically) &quot;greyed-out&quot; ifdisabled.  When the user presses the button using a mouse, the model is notifiedby the UI that the button has been pressed.  On the condition that the button isnot disabled, the model will, in turn, notify the UI to draw the button in itsdown position.  The model will also notify any interested parties that it is hasbeen pressed, and they can respond as appropriate.  Note that if the button isdisabled, even if the UI informs the model that a press operation has takenplace, the model will do nothing.</P><P>In this way, Swing components decouple their user interface from theirinternal states.  The model represents what the component &quot;really is&quot;,abstracting away from its presentation details, and thus, is a constant.  Thatis, the model will remain the same across different user interface designs.  Theimplication is that developers can define <EM>exactly</EM> how each componentpresents itself and reacts to user inputs.</P><P>In addition to simple decoupling of presentation from model, Swing goes onestep further:  one can remove a given look and feel from a component, andreplace it with another.  This is the &quot;pluggable&quot; nature Swing'sapproach to the user interface.  For example, a designer can decide that,overall, the user interface will have a Windows 95 motif, but push buttons willlook and behave like Macintosh buttons.  This can be accomplished by installingthe Macintosh button look and feel whenever a button is needed; otherwise theWindows look and feel is installed.</P><H3>Multiplexing</H3><P>Finally, Swing has the capability of running multiple look and feelssimultaneously.  This is its so-called &quot;multiplexing&quot; feature.  Theidea here is that a particular look and feel is considered the primary interfacethat users will use.  There may be, in addition, a set of auxiliary userinterfaces, running cooperatively with the primary interface, that providealternate means of interacting with the software.  These secondary look andfeels are &quot;pluggable&quot; in the same way that the primary is with theresult tha all of the user interfaces, primary and auxiliary, are hooked intothe same model at the same time.</P><H2>Accessibility via the Pluggable Look and Feel</H2><P>The separation of the UI from model is a solution to the probelm ofaccessibility with respect to Java-based applications.  First, due to thisseparation, it is possible to create a user interface with a specific kind ofdisability in mind.  Secondly, since there is provision for multiple userinterfaces, more than one type of UI can talk to the same model at one time.This allows for the employment of multi-modal interfaces.  For example: When theuser performs an action on a button, one UI would notify the user of changes tothat button's state in a visual way, while another UI would simultaneously useauditory cues. In addition, on the &quot;feel&quot; side of the equation,because the model is separate from the UI, it does not care which UI the useremploys to control the system.  Thus, pressing a button with a mouse, keystroke,pointing device, or even a verbal command will not change the way that itbehaves when pressed. The model only listens for the change to itself, and doesnot discriminate about the &quot;how.&quot;</P><P>The term &quot;Look and Feel&quot; can be somewhat deceptive in that itimplies the need for the user to &quot;look&quot; at the screen with his/hereyes. In fact, the &quot;Look&quot; portion of a &quot;Look and Feel&quot;refers to the modality used to present information to the user. In the audiolook and feel described below, the &quot;Look&quot; is auditory. This means thatall of the necessary information is conveyed through a combination of verbalconfirmations and prompts, which are often preceded or accompanied bydistinguishing sound effects. Conversely, the &quot;Feel&quot; portion of a &quot;Lookand Feel&quot; identifies the method employed by the user to control the system.In this case, the &quot;Feel&quot; is mouseless or keyboard only.  In terms ofaccessibility, the auditory feedback (Look) allows persons with a visualdisability to use their ears to in understanding the interface.  Creatingkeyboard equivalents makes the interface accessible for individuals using voicerecognition software, alternative keyboards, and other types of keyboardreplacements.  Indeed, an audio look and keyboard feel allows any user to usesoftware without paying strict attention to the visual display, and usekeystrokes where they feel that is more convenient than the mouse.</P><H2>Audio Look and Feel:  Design</H2><P>In brief, the audio look and feel uses audio sequence to present  Swingcomponents and their contents, and uses keyboard control to explore andmanipulate them.  The audio used includes both verbal (speech) and non-verbalsounds.</P><H3>Look</H3><H4>Speech</H4><P>Speech is an essential part of the Look and Feel for the reason that it isimpossible or impractical to sonify some things in any other way.  While aspecific sound effect may well communicate the presence of, for example, a checkbox, the purpose of that check box cannot be so communciated.  Check boxes comewith a label that indicates what that check box is for.  It is impossible tocommunicate that label other than via speech.  In addition, the spectrum ofqualities that objects can have can lead to a large number of sound effects withwhich to communicate them.  Too many sound effects lead to memory demands thatcannot be handled by any user [**?T.V. Raman reference:  &quot;the best kind ofUI is one that you don't notice&quot; i.e. don't have to think about too much?].  Instead, such qualities would better be presented via speech.  As a concreteexample, consider text.  Text can have a number of attributes such as colour,font family, font stytle, and so on.  The differences between fonts and colorscan be very subtle, and it would require an almost infinite number of sounds torepresent them.  Stating &quot;ten point&quot; or &quot;times roman&quot; or &quot;bold&quot;is far and away a better way to communicate this kind of information.</P><H4>Non-verbal Audio</H4><P>Speech, however, does have its limitations; notably its length or duration. Users do not want to spend  time listening to a lecture on a check box.  Hence,whenever the information can be communicated using a non-verbal sound, thattechnique is preferred.  The audio look and feel uses non-verbal sound toindicate:</P><UL><LI>the type or role of component (e.g., check box, push button, slider).</LI><LI>whether it is disabled.</LI><LI>its current state (e.g., checked, pushed, slider value).</LI><LI>that one has navigated to the component, as opposed to changing its state.</LI><LI>that one is activating or manipulating the component (e.g.,checking/unchecking the check box, pushing the button, or sliding the slider).</LI></UL><P>The sound effects for the UI are a combination of metaphoric andnon-metaphoric sounds. Metaphoric sounds are based on real world sounds thataccompany well known activities.  This is done in the hope that these will makethe component and concepts associated with them easier to remember andunderstand. For example, the action of cutting a selection from a document isassociated with the sound of scissors cutting.  &quot;Cutting the selection&quot;is a very specific action. Neverthess, metaphors can also be employed torepresent more generalized tasks. For example, the sound effect for opening amenu or tabbed pane is the sound of a drawer opening.  Like each of thesecomponents, a drawer is one of a group of drawers each of which can be opened. Several items are typically stored in the same drawer and categorized accordingto some type of organizational system.  Using a drawer opening sound effectcommunicates to the user that they have &quot;opened&quot; the menu, or &quot;opened&quot;the tab.</P><P>Metaphoric sounds are appealing for two reasons. First, because they aredesigned for commonly encountered tasks, they are heard repeatedly by the useras s/he interacts with the system. This means they must be distinctive, buteasily recognizable, and bring a degree of consistency of experience. Second,people from several different cultures are able to understand the metaphor and,thus, are able easily recognize the functions served by such objects.  (Note: there is a cultural issue here -- not all cultures will understand the metaphor. The same can be said for iconic communciation, however).</P><P>Non-metaphoric sounds are beeps, buzzes, melodies, and clicks that arearbitrarily assigned to components and their state(s).  They provide the userwith feedback that does not have an immediate analogy to the thing that theysonify.  What is the sound of a tree, or list, or save action?  In addition, inthe interest of consistency, it is better to provide the same sound across manycomponents to indicate the same feature.   For example, a buzzer noise[**Karen/Lake/me:  check this description; it may now be a &quot;ping&quot;] isused to cue the user when a component is not available or disabled.  Since thesame audio is used for all components, it cannot be metaphoric.  Nonetheless,the consistency of use allows the user to know &quot;immediately&quot; that saidcomponent is not going to do anything for the present.</P><P>Above, it was noted that a reason for using speech is that it can be used toquickly communicate information in some cases.  A similar principle applies withrespect to non-verbal audio.  An effort has been made to create and use soundsthat are simple, uncluttered, and efficient.  For these reasons, the non-verbalaudio cues were chosen to be short in duration, while remaining distinctive.</P><P>In summary, sound sequences that are complicated by multiple parts andextended durations have been avoided, and sounds that represent same or similartasks are recycled wherever possible. Re-using sounds often involves thetransfer of identical sound sequences from one part of the interface to another.However, it can also involve the use of characteristics like melodic structureor timbre to make fine tuned distinctions between components in the samecategory. (See the RECYCLING SOUNDS section below.) It is hoped that thesetechniques will make the interface efficient, and easy to learn and remember,and consistent.</P><P>[***Note to self:  there are a number of design principles floating aroundin the above:  quickness, efficiency, meaningfulness, consistency (redundancy),and ???.  It's not clear that they are particularly well presented.  Revisit.]</P><H4>Sound Sequences</H4><P>The audio look and feel has been designed with a particular learningstrategy in mind.  Where non-verbal audio has been included, it has been placedintentionally at the beginning of feedback sequences, and used to communicate avariety of things.  To review, these include successful navigation from onecomponent to another, identify the component of focus, verify that the state ofsome component has changed, and announce the presence of system-based requestssuch as warnings and alerts.  That these non-verbal sounds occur early in thefeedback sequence, coupled with the facts that they are shorter in duration thanspeech and easier to process, encourages users to become more dependent on thesesounds and less dependent on the verbal cues.  It is hoped that the user willhear the sounds and understand their meaning without needing to hear all of theverbal portion of the sequence.  Users are allowed to press a key sequence tocause the feedback to cease when they have understood the gist of the message --during an interim period, that can hear as much or as little of the verbalfeedback as necessary.  In the long run, users will be able to adjust the levelof verbal prompting in such a way as only those speeches that are absolutelynecessary will be emitted, while those that are redundant with the non-verbalaudio cues are eliminated.</P><P>The general approach of the audio look and feel, in terms of feedback, is toprovide users with sequences of sound effects and speech.  Broadly speaking,there are two kinds of feedback sequences:  navigation and activation.</P><P>As conventional screen reading technology demonstrates, making thedistinction between navigation and activation is crucial for users who rely onauditory output. These individuals cannot obtain information about the layoutand contents of the screen by glancing at the monitor; they cannot use theireyes to determine their available options for acting on the interface.Therefore, it is required that these essential details be conveyed to allowusers to make a decision about how and where to act.  If the component of focuswere automatically activated, the user would run the risk of initiating a veryundesirable or unproductive sequence of events without any prior warning. Inthat case, the interface would be controlling the user's behavior when it shouldbe the other way around.  In terms of auditory feedback, one sequence is usedfor navigation, and another for activation.</P><P>In addition to their being two major kinds of feedback sequences, theinformation that can be conveyed about a component is not all delivered &quot;atonce&quot;.  It would be foolish to do so, since a complicated component couldresult in a very long auditory diatribe describing every last feature of thatcomponent.  To avoid this, the audio look and feel partitions the informationinto separate sequences.</P><P>When a component initially gains focus, the user is given the essentialinformation about that component:</P><OL><LI>that focus has moved to that component.</LI><LI>that the component is disabled if it is, in fact, disabled.</LI><LI>the type and typical state of that component.</LI><LI>the name of the component.</LI></OL><P>If they so desire, they can request additional information by asking for atool tip, or requesting &quot;extra&quot; information.  The tool tip will bedelivered if the component has one.  What the &quot;extra&quot; information isdepends on the component in question.  For a menu item, the &quot;extra&quot;information is limited to the hot-key and/or short cut key that activates themenu.  For a list, the &quot;extra&quot; information is the items that arecurrently selected, if any.  This last example illustrates the rationale forpartitioning the information.  Users would not want to here the selection set ofa list whenever they navigated to it, but would want to know whether anythingwas selected.  After finding that out, they can then, at their leisure, ask thelist for that extra information.</P><P>[*** Dena's original thoughts on this.  More of this stuff needs to beincorporated in the above discussion on partitioning...]:  This Look and Feel isequipped with keyboard commands that allow the user to obtain this feedback.(Example: The &quot;Where Am I?&quot; command provides higherarchical data toillustrate to the user the path that was taken to reach this particulardestination or point of focus. The &quot;Request For Tooltip&quot; command isspecific to Toolbar Buttons, and supplies the user with information about theButtons' states. Finally, the &quot;Extra Information&quot; command identifiesany Hotkeys or Shortcuts that may be associated with the component.)  	Extrainformation can also be helpful when things like Sliders, Tables, or slow-moving progress bars are encountered. This command can provide the user withinformation such as the upper and lower limits associated with the Slider, thenumber of columns and rows in a Table, and the percentage of the Progress Barthat has already been filled. Most popular screen reading programs are able toobtain similar kinds of contextual information for their users. The power of aPLAF, however, is the fact that it can obtain specifics about a component thatare outside the limits of a screen reader. The fact that the UI is able to speakdirectly to the Model allows it to present extremely detailed kinds ofinformation to the user; characteristics of color and texture, size in pixels,state... This information is partitioned through the use of separate keyboardcommands that tell the Model what specific things the user wishes to know aboutit at a given time. (See the GETTING MORE INFORMATION section below.) 	Thebenefit to partitioning the information is flexibility for the user. S/he isable to control the type and amount of component-specific data received. It alsoallows him/her to make the decision about when the receipt of such informationis appropriate or helpful. Finally, it makes for an interface that is moreefficient. If the user is extremely familiar with an application, s/he may notrequire extra information about a component or its location. If this material ispresented by default, it effects the productivity of the user who already hasthese things memorized.</P><H3>Feel</H3><P>As mentioned above, the &quot;Feel&quot; of a &quot;Look and Feel&quot; isthe method used to control the system. This is how the user explores/navigatesthe interface, and activates/manipulates it. For the audio look and feel,control is solely via the keyboard.  In other words, the user can move about thecomponents and explore their configuration as well as activate or manipulatethem using only key presses.</P><P>In addition, Swing allows for some special abilities with respect tokeyboard navigation.  This feature of Swing is used by the audio look and feelto improve the speed of navigation.  One of these is &quot;warping&quot; focus,and the other is first key navigation.  These are described more fully in thefollowing.   [** Can someone confirm or deny that the following is a differentfrom what a screen reader can do?  That is, do screen readers allow one to &quot;warp&quot;?Or do a first-key search?]</P><H4>Warping</H4><P>Most of the navigation available to the user can be termed &quot;incremental&quot;. That is, at any given time, the user is at one point in the interface, and canmove up, down, left or right to the &quot;next&quot; location.  Such navigationcan be tedious, especially when the user knows where they want to go.  Toalleviate some of this tedium, the audio look and feel allows for warping thekeyboard focus to well defined areas of the user interface.   Warping is theability to enter a key press and have focus move directly to the component ofinterest.  Currently, the audio look and feel allows warping for thesecomponents:</P><UL><LI>menu bar</LI><LI>tool bar</LI><LI>desk top</LI><LI>text field</LI><LI>password field</LI><LI>text area</LI><LI>text pane</LI><LI>editor pane</LI></UL><P>[**Note to Anastasia/Karen:  can you provide something regarding navigationwithin text by word/paragraph/sentence.  I think this could be classified as akind of warping]. </P><H4>Contextual Search</H4><P>Another technique used to speed up navigation is a kind of contextualsearch.  In this case, the &quot;context&quot; is a component that contains afinite group of items.  Rather than incrementally moving from item to item, ifthe user knows what a particular item is called, s/he can locate that itemquickly by attempting to match its first letter. This kind of navigation issometimes referred to as first-key navigation.  For example, a menu contains anumber of menu items.  Using contextual search, the key presses are used to findan item that begin with the key pressed, and move focus to it.  Subsequent keypresses continue the search to the next item that begins with that letter untilthe end of the group is encountered at which point the search continues with thefirst item.  The audio look and feel supports contextual search for thesecomponents:</P><UL><LI>menu bar</LI><LI>menu</LI><LI>tool bar</LI><LI>tabbed pane</LI><LI>list</LI><LI>combo box</LI><LI>desk top (with respect to finding windows and icons on the desktop).</LI></UL><P>By combining warping and contextual search, users can speed up navigationconsiderably.  Here is a concrete example:  the user has selected some text andwishes to activate the &quot;Cut&quot; menu item in the &quot;Edit&quot; menu:</P><OL><LI>Key press to warp to the menu bar.</LI><LI>Contextual search to find the &quot;Edit&quot; menu.</LI><LI>Key press to open the &quot;Edit&quot; menu.</LI><LI>Contextual search to locate the &quot;Cut&quot; menu item.</LI><LI>Key press to activate the &quot;Cut&quot; menu item.</LI></OL><P>The same sequence can be accomplished using the standard incrementalnavigation key strokes, but many more key strokes are needed to acquire thatparticular menu item.</P><P>Despite the presence of such enhancements, an effort was made to base someof the keyboard commands on those used by some of the more popular screenreading packages, and those that have already been incorporated in the OS. [** &lt;==is this true?  the bit about the &quot;OS&quot; key strokes?  I guess withrespect to activation (ie., &quot;return&quot;)**]. This was done to accommodateusers who may have had previous experience with such assistive technology andthe use of OS specific keyboard equivalents. These include the method ofactivating components via the keyboard, keyboard equivalents used for textselection and modification, and the ability to gain contextual help andinformation about the interface. (See the KEYBOARD EQUIVALENTS section below.)	As noted above, the primary reason for adding keyboard control of any kind isto enable someone, who cannot use a mouse, to access the interface independentlyand efficiently. This efficiency is further increased if the commands are easyto learn and remember. This can be accomplished by employing the followingstrategies: incorporating a letter that can be directly associated with thecurrent command whenever possible, (see the KEYBOARD EQUIVALENTS section below), employing toggles that can be turned on and off via the keyboard, and using thesame key stroke for similar functionality across many components. Toggles canassist the user with tasks like performing text actions with greater speed andprecision. Information can be obtained about active and inactive textattributes, and the modes associated with them can be turned off and on withoutguesswork or having to move to the Menubar. (See the TOGGLES FOR TEXTACTIONS/ATTRIBUTES section below.)  Using the same key stroke introducesconsistency in &quot;feel&quot;.  For example, the key strokes used to navigatea menu item-by-item are the same as those used to navigate a tabbed panetab-by-tab.</P><H3>Audio Look and Feel vs a Conventional Screen Reader</H3><P>The speech portion of the audio look and feel differs from a conventionalscreen reader in the following ways.  A screen reader speaks the visualpresentation of a program, and interprets it for the user. It is essentially thego-between for the user and the mainstream application. Often, the screen readeris its own, completely separate program, with one overall set of commands thatmust be transferred across various types of mainstream software packages;spreadsheets, word processors, internet tools, and so on.  To be blunt, separateversions of OutSpoken, JAWS, and Screen Reader have not been created for Lotus,Microsoft Office, Netscape Navigator and other applications.  Exceptions to therule include Arkenstone's Open Book software, and Productivity Works, PWWebSpeak. These products have their own built-in screen readers and have beenspecifically designed with visually impaired users in mind.</P><P>A multiplexing pluggable look and feel architecture allows for thesimultaneous presentation of visual and audio information for the UI. This meansthat the speech output need no longer be contained in a separate application,and a third party interpretation of the visual interface is no longer necessary.</P><P>Another difference is that screen reader packages do not include supportingsound effects. They are simply supplemented by whatever sounds are currentlyloaded on the OS.   An example is the choices for sound themes found in Windows.Given the fact that the sounds are part of the OS, and the speech is beinggenerated by an application that has been installed on top of the OS, the twocan often interrupt or over-write one another. For a pluggable look and feel, itis possible to integrate speech and non-speech audio into a collective method ofresponding to changes made to the UI. In other words, direct presentation innon-mainstream modalities of mainstream applications is now possible. Conflictsoccur far less frequently, due to the fact that the implementation of these twoforms of auditory information can be carefully programmed. However, if anattempt is made to string several sounds together, problems with sound overlapor system overload may result.</P><H2>Benefits of an Audio Look and Keyboard Feel</H2><P>	An audio &quot;Look&quot; and keyboard &quot;Feel&quot; is helpful tocomputer users with visual or motor impairments, as well as individuals with avariety of learning disabilities. Audio feedback is essential for people who areunable to physically look at or cognitively process information in a visualformat. In the same way, replacing the mouse with a series of keyboardequivalents is vital to accessibility, as mouse-driven applications presentinsurmountable barriers to several groups of computer users with alternativeinput requirements. Mice are inaccessible to blind and partially sighted usersbecause they cannot use their eyes to track the pointer as it moves across thescreen. They are not usable by persons with mobility constraints because they donot have the fine motor control or muscular strength to manipulate smallobjects. Finally, they can cause problems for users who have difficulty withhand-eye coordination or other spacial or visual processing abilities. </P><P>Furthermore, a Java-based audio look and feel can be easily transferred fromplatform to platform.  The benefit is that the characteristics of the interfacewill remain consistent regardless of the operating environment. A button that isdisabled sound the same regardless of whether or not the user is working in aWindows 95/98 or Macintosh environment.  In addition, it is hoped thatindividuals using Personal Java devices with small or non-existent screens willeventually be able to benefit from this technology. In this market of portable,Web-connectable devices, consumers may be frequently placed in situations wherethey are required to access information while performing a task that demandstheir undivided visual attention. An example would be a commuter who wishes toaccess his/her email while driving to or from the office.</P><H2>Appendix of Methods</H2><P>[*** Warning:  other than putting the following into HTML, I have not reallyedited it.  So I don't know where it really belongs.**]</P><H3>A) KEYBOARD EQUIVALENTS</H3><P>Examples of pre-existing OS commands for the Windows environment include:CTRL+C = Copy and CTRL+X = Cut. Examples of commands common in popular screenreaders include: using the ARROW keys for navigation of text, Menus, and Lists,using the ENTER key to activate components of focus, and using keystrokes to askfor information about specific and general location, available commands, andinformation about text attributes. Examples of keyboard equivalents in this Lookand Feel that contain a letter that is associated with the desired commandinclude: CTRL+SHIFT+E = Extra Information and CTRL+SHIFT+T = Request For TooltipFor Toolbar Button. Both Take information out of the Models for their respectivecomponents and present it to the user.</P><H3>B) TEXT-BASED COMPONENTS</H3><P>The five kinds of text-based components are: Text Fields, Password Fields,Text Areas, Text Panes, and Editor Panes. Keyboard equivalents have beendesigned to allow for easy navigation to each of the above. They include:SHIFT+F11 = Warp To Text Field, SHIFT+F10 = Warp To Password Field, SHIFT+F12 =Warp To Text Area, SHIFT+F9 = Warp To Text Pane, and SHIFT+F8 = Warp To EditorPane.</P><H3>C) TEXT SELECTION</H3><P>Another feature of text navigation is the ability to select it. This Lookand Feel offers the standard set of key commands for text selection, such asCTRL+A or the SHIFT+ARROW KEY sequences. A couple of extras have also beenadded. These are: F4 = Read Currently Selected Text, CTRL+PLUS/MINUS = Add OrRemove From List Of Selected Items, and CTRL+SHIFT+E = Read The List Of SelectedItems In A List Or Combo-Box. Finally, if the user presses CTRL+SHIFT+R while inan Editable Combo-Box, this command will toggle him/her between Selection Modeand Edit Mode.</P><H3>D) TOGGLES FOR TEXT ACTIONS/ATTRIBUTES</H3><P>Text actions usually involve work with text attributes. Keyboard toggleshave been added to this Look and Feel to represent those attributes that aremore commonly used. Examples include: CTRL+B = Bold On/Off, CTRL+8 = ItalicsOn/Off, and CTRL+U = Underline On/Off. These are in addition to the set ofstandard toggles like CAPSLOCK and NUMBERLOCK Text attributes such as font andcolor can also be modified via the keyboard, but are only sonified via speech.</P><H3>E) TEXT NAVIGATION</H3><P>Navigation through blocks of text can be accomplished via the ARROW keys.However, each command of this type has also been partnered with its own FUNCTIONKEY equivalent. For instance: CTRL+RIGHT ARROW and F7 = Read Next Word andCTRL+LEFT ARROW and F5 = Read Previous Word. Additionally, pressing F12 willcause the document to be read in its entirety, F8 will allow the user to herethe text from the cursor to the end of a document, and ALT+F1 toggles betweenWord and Character Echo Mode while reviewing text.</P><H3>F) NON-TEXT NAVIGATION</H3><P>Non-text-based components include: Menubars, Toolbars, Tabbed Panes, Lists,Combo-Boxes, and the Desktop. The user can move onto the Menubar by pressingCTRL+M. S/he can then navigate between the levels in the hierarchy by using theUP/DOWN ARROW keys. Levels include the actual Menubar, the Menu Titles, thelevel where Menus are opened, and the Menu Options themselves. Once a level hasbeen opened, the user is able to move among the choices in that level by usingthe CTRL+RIGHT/LEFT ARROW keys. In other words, the user is able to move up anddown from one level to another, and left to right within a given level. Thebuttons on the Toolbar can be navigated in exactly the same way, except that theToolbar is only one level deep.   	Additionally, within a given level of themenu system, pressing a key will navigate to the first item whose first lettermatches the one typed. Subsequent presses of the same letter will navigate tothe next matching item... The search will wrap until one of the items isselected, or until the cycle is broken with the selection of another letter orCTRL+RIGHT/LEFT ARROW or UP/DOWN ARROW sequence. This alphabetically-basedsearch feature also works for Tabbed Panes, Toolbar Buttons, in Lists, whilemaking selections in a Combo-Box, and on the Desktop.  GETTING MORE INFORMATION	The key sequence CTRL+SHIFT+H has been defined to allow the user to requestinformation about where s/he is in the application. For example: Menubar - FileMenu - Save. The CTRL+SHIFT+T sequence deals exclusively with the Buttons foundon Toolbars. It tells the user whether they are &quot;disabled&quot; or &quot;pressable.&quot; Another way to obtain the particulars of an item of focus include theCTRL+SHIFT+E or &quot;Extra Information&quot; command.</P><H3>G) RECYCLING SOUNDS</H3><P>In some instances, it has been possible to re-use the same sound in a numberof places. Examples include: the clicking sound that was used to conveysuccessful navigation between Menubar and Toolbar items, and the scribble noisethat accompanies all areas where alphanumeric data can be entered by the user.	In cases where components have been grouped together, other attributes havebeen used to make distinctions. Items have been categorized according tofunctionality rather than appearance. For instance, Radio Buttons, Checkboxes,and Toggles are considered to be a group because the information that the userneeds to know is whether they are active/inactive, checked/unchecked, on/off.The same melodic pattern is used to indicate the active, checked, or on statusset, and another melodic pattern is used to represent the inactive, unchecked,or off status set. The components, themselves, are distinguished via tambre ortype of musical instrument.  	Although Buttons could be considered similar, theyhave not been put into this group for two reasons. First, they can have statesother than pushed/unpushed. For instance, disabled. Second, Buttons typicallyappear as part of Dialog Boxes. Dialog Boxes are modal, which means that theuser must act on them before they will disappear. This action usually requirespushing a Button. In the case of Radio Buttons, Checkboxes, and Toggles, theuser can verify his/her choices after the fact. Once a Button has been pushed,the user cannot go back into the Dialog Box to re-check the Button's status. TheDialog simply disappears.</P></BODY></HTML>