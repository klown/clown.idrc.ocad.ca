<!DOCTYPE HTML PUBLIC "-//SoftQuad//DTD draft HTML 3.2 + extensions for HoTMetaL PRO 3.0 19960802//EN" "hmpro3.dtd"><HTML><HEAD><TITLE>Audio Look and Feel:  Architecture</TITLE></HEAD><BODY BGCOLOR="#ffffff" TEXT="#000000"><CENTER><H1>Audio Look and Feel:  Architecture</H1><BR><P>Joseph Scheuhammer, Anastasia Cheetham, and Allen Forsyth.</P></CENTER><H2>Introduction</H2><P>This is a technical document that describes the architecture of a pluggableaudio look and feel using Swing.</P><P>Some prelimimaries:  First, the audio look and feel is an auxiliary look andfeel in the sense that it does not replace the graphical user interface. Indeed, the visual and auditory interfaces run concurrently and cooperatively. Secondly, it is irrelevant to the audio interface as to which particular visualuser interface is &quot;in effect&quot;, be it Metal, Windows, Motif, orMacintosh.  That is up to the user and/or application developer.  In other words,the audio look and feel works regardless of the visual user interface.</P><P>What, precisely, does it mean to have an auditory &quot;look&quot; and &quot;feel&quot;? With respect to the &quot;look&quot;, Swing components are presented using asequence of speech and non-speech audio.  In terms of user control (the &quot;feel&quot;),the components are manipulated solely via the keyboard.  Note that this does notpreclude their manipulation by the mouse; however, mouse control is the provinceof the visual look and feel.  No mouse control is defined for the audio look andfeel, although appropriate audio feedback is provided when mouse control altersthe state of a component.  Thus, the audio look and feel is more preciselydescribed as an auditory &quot;look&quot; and keyboard &quot;feel&quot;.</P><H2>Architecture</H2><P>The architecture of the audio look and feel has two main parts.  The firstis a mechanism that is responsible for generating audio feedback.  The<CODE>ReportGenerator</CODE> assembles a sequence of speech and non-speechaudio appropriate to a context, and that sequence is then passed to an audiointerface for presentation.</P><P>The second aspect of the system is that it is event driven.  The type of sequence generated is determined by an event, and, more importantly, a specifictype of event.</P><P> For example, suppose a user has pressed the key to uncheck a check box, andthat the check box has responded accordingly.  The user is notified of the newstate via an auditory sequence.  To the user, it will appear as if the feedbackwas an immediate consequence of their key stroke.  While that is the ultimatecause, the connection between the key stroke and the auditory feedback is lessdirect.  Internally, the keystroke will eventuate in a change in the check boxstate; the check box, in turn, emits an <CODE>ItemEvent</CODE>.  It is thatevent that is the immediate cause of the audio feedback.  The is accomplished bymaking the<CODE>ReportGenerator</CODE> an <CODE>ItemListener</CODE>, and adding it assuch to the check box in question.  It is this event/listener combination thatcues the auditory feedback.</P><P>That the feedback is event-dependent instead of key stroke-dependent, has animportant consequence.  It means that feedback occurs in response to theappropriate event, regardless of user input.  In other words, if some internalprocessing causes a component to change state, and that change entails thecomponent's emitting the appropriate event, then audio feedback is output. Users are notified of the changes even if they were not directly responsible forthem.  For example, navigation feedback is typically output when focus istransferred to a component and that is generally the result of the user pressingsome key.  However, if the focus is gained by some means other than a keystroke, then the same auditory feedback will occur.</P><P>In terms of Swing's pluggable look and feel, audio feedback is realized bycreating a class that extends the basic functionality of the <CODE>ReportGenerator</CODE>,and implements the relevant listener interfaces.  A reporter/listener instanceis then added to a component when its user interface is installed.</P><P>In the next section, the report generator aspect of the system is describedin more detail.</P><H2>Report Generator</H2><P>To a first approximation, two kinds of feedback are generated.  One istermed &quot;navigation&quot;, and the other, &quot;activation&quot;.</P><P>Navigation means that the user has &quot;directed their gaze&quot; at aspecific component, and desires to know about it.  They are inspecting thecomponent, looking to see what it is and what it can do.  There is no intent toalter that component; rather, they merely desire to know how it is currentlyconfigured.</P><P>On the other hand, an activation report is generated in response to someaction that does alter the component.  This occurs when users want to change, ormanipulate, the component in question  Buttons can be pushed; check boxeschecked or unchecked, and menu items selected.  All these interactions with acomponent are grouped under the rubric of &quot;activation&quot;.</P><H3>Navigation</H3><P>Navigation reporting itself comes in a number of flavours.  The most commonof these occurs when the user navigates to the component for the first time.  Inthis case, the feedback conveys a sense of movement, and provides a descriptionof the component to which one has just moved.</P><P>A second kind of navigation report occurs when the users have forgottenprecisely where they are, and desire a re-cap of that information.  In essence,they are asking &quot;where am I?&quot;.</P><P>Also, there is often a tool tip associated with a component, and users canrequest that information.  In a similar vein, there may be other &quot;extra&quot;information associated with the component, which users can ask for at theirleisure.  An example of this extra information is a hot key on a menu item.  Animportant point about these latter two kinds of information is that they are notoffered spontaneously to users when they navigate to a component.  The rationaleis that doing so makes the feedback relatively long.  Instead, the audio lookand feel regards this kind of information as an aside that is available, butonly upon request.</P><H4>Navigation-to Report</H4><P>The &quot;navigation-to&quot; feedback sequence occurs after the user hasentered a key stroke to move to a new component.  The purpose of the sequence isto confirm the move and to describe where it is the user has &quot;landed&quot;.</P><DL><DT>1.  Navigation sound effect.</DT><DD>A sound effect to indicate that navigation has been accomplished.</DD><DT>2.  Disabled sound effect.</DT><DD>A sound effect to indicate that the component is disabled.  This soundeffect is made only if the component is disabled.  No sound effect is played ifthe component is enabled.</DD><DT>3.  Role/state sound effect.</DT><DD>A sound effect to indicate the type of component, and its current state. This sound effect is unique to the component, and is associated with its mosttypical state.  For example, for a check box, this sound effect would indicatethat it is a check box, and whether it is checked or not.</DD><DT>4.  Name of component</DT><DD>A speech that gives the name or textual label of the component.  If nonecan be found, the speech defaults to &quot;no label&quot;.</DD><DT>5.  Type of component</DT><DD>A speech to describe the component's type, or role.  For example, the typeof a check box is &quot;check box&quot;.  If none can be found, the speech is &quot;unknowncomponent&quot;.</DD><DT>6.  State of component</DT><DD>A speech to describe the component's most typical state.  Using check boxas the example again, this speech is either &quot;checked&quot; or &quot;unchecked&quot;. If no state can be deduced, the speech is &quot;unknown state&quot;.</DD><DT>7.  Disabled speech.</DT><DD>A speech to indicate that the component is disabled if it is so.  No speechif given if the component is enabled.  The speech is &quot;disabled&quot;.</DD></DL><HR><P><STRONG>Note:</STRONG>  The sequence is configured such that the soundeffects occur before the speeches.  The reason is so the system can be sensitiveto users' expertise with the interface.  As &quot;novices&quot;, users want tohear the entire sequence.  As they learn the meaning of the sound effects, thosebecome sufficient in describing the state of affairs.  As &quot;experts&quot;,users no longer require all of the speeches and come to rely on the soundeffects.  Although currently not implemented, there will be both a provision tocancel the auditory feedback via a key stroke, and the ability to set apreference that only essential speeches (e.g., name of component) be output.</P><HR><P>Here are three concrete examples of the navigation report for <CODE>JCheckBox</CODE>,for<CODE>JMenuItem</CODE>, and for <CODE>JList</CODE>.</P><H5>JCheckBox</H5><OL><LI>Navigation-to sound effect.</LI><LI>Disabled sound effect, if disabled.</LI><LI>Check box sound effect.<UL><LI>Three tones.  The first and last are the same pitch.  The middle is lowerif the check box is checked, higher if unchecked.  The timbre of the tonesindicates that it is a check box.</LI></UL></LI><LI>Name of the check box.</LI><LI>Speech to indicate that is a check box.</LI><LI>Speech to indicate whether it is checked or not.</LI><LI>Disabled speech, if disabled.</LI></OL><H5>JMenuItem</H5><OL><LI>Navigation-to sound effect.</LI><LI>Disabled sound effect, if disabled.</LI><LI>No role/state sound effect.</LI><LI>Name of the menu item.</LI><LI>No role speech.</LI><LI>No state speech.</LI><LI>Disabled speech, if disabled.</LI></OL><P>Commentary:  Note that a lot of the navigation report is &quot;missing&quot;for menu items -- there is no role/state sound effect, nor a role or statespeech.  The rationale is that once in a menu, users know where they are andthat they are navigating from menu item to menu item.  They do not need to hearrepeatedly that they have just moved to a new menu item.</P><H5>JList</H5><P>There are, in fact, two versions of &quot;navigation-to&quot; feedbacksequences  for lists.  The type generated depends on whether the user hasnavigated to the list for the first time versus navigating among its items.</P><OL><LI>Navigation-to sound effect.</LI><LI>Disabled sound effect, if disabled.</LI><LI>List sound effect, if just navigated to the list; no sound effect ifnavigating the items.</LI><LI>Name of list, if just navigated to the list; name of the list item ifnavigating the items.</LI><LI>Speech to indicate that is a list., if just navigated to the list; speechto indicate that it is a list item if navigating the items.</LI><LI>If the user has just navigated to the list, then a speech describing thenumber of items in the list, number currently selected, and the current itemname.  If navigating the list items, and the current item is selected, then aspeech stating that the current item is selected.<UL><LI>Note:  the current item is tracked by the audio user interface independentof the primary look and feel.  It is frequently identical to the &quot;leadindex&quot;, but need not be.</LI></UL></LI><LI>Disabled speech, if disabled.</LI></OL><H4>Where Am I? Report</H4><P>A user interface must allow that users are not always concentrating solelyon the components that they are interacting with.  They may be composing adocument and thinking predominantly about the best way to express an idea; orthe phone may ring and they must deal with that distraction.  Whatever thecause, there will occur situations where users will not be sure of what theywere doing before they were distracted and want a statement as to where they arewithin the user interface.  In order to handle this situation, the audio lookand feel provides a key stroke to elicit a &quot;where-am-I?&quot; report.</P><P>The &quot;where-am-I?&quot; report is, to a first approximation, identicalto the &quot;navigation-to&quot; report.  The main difference is the lack of theinitial movement sound effect.  In addition, in some cases, a larger context isprovided wherein the relevant parent of the component is also stated.  How,exactly, a &quot;where-am-I?&quot; report is configured is determined by thecomponent at hand.  As examples, here are the &quot;where-am-I?&quot; feedbacksequences for <CODE>JCheckBox</CODE>,<CODE>JMenuItem</CODE>, and <CODE>JList</CODE></P><H5>JCheckBox</H5><OL><LI>Disabled sound effect, if disabled.</LI><LI>Check box sound effect.</LI><LI>Name of the check box.</LI><LI>Speech to indicate that is a check box.</LI><LI>Speech to indicate whether it is checked or not.</LI><LI>Disabled speech, if disabled.</LI></OL><H5>JMenuItem</H5><OL><LI>No initial disabled sound effect, even if disabled.</LI><LI>No menu item sound effect.</LI><LI>A speech describing the &quot;path&quot; through the menu system.   Forexample, if the user was focused on the &quot;Open...&quot; menu item of the &quot;File&quot;menu, they would hear, &quot;Menu bar, File, Open&quot;.</LI><LI>No menu item speech.</LI><LI>No specific state speech.</LI><LI>Disabled speech, if the menu item is disabled.</LI></OL><H5>JList</H5><OL><LI>Disabled sound effect, if disabled.</LI><LI>List sound effect.</LI><LI>Name of list.</LI><LI>Speech to indicate that is a list.</LI><LI>Number of items in the list, number currently selected, and the currentitem.</LI><LI>Disabled speech, if disabled.</LI></OL><H4>Extra Information</H4><P>Extra information is typically short cut (a.k.a. &quot;mnemonic&quot;) keysand hot keys.  If no such keystrokes are available, then nothing is reported. For components where short cut and hot keys do not make sense, some other aspectof their current state is provided.  For example, the extra information for<CODE>JList</CODE> is a spoken list of its currently selected items.  Note thatthis is information <EM>not</EM> provided by the &quot;where am I?&quot;report.</P><H4>Tool Tip Report</H4><P>The tool tip report is simply the text of the component's tool tip spoken. If their is no tool tip, then the speech &quot;no tool tip&quot; is spoken.</P><H3>Activation</H3><P>The activation report is an auditory sequence that indicates what has justbeen activated or manipulated.  There are two kinds of activation report.  Thefirst is a general sequence that is used for a specific type of component.  Itis general in the sense that is it used for all components of that type.  Thus,for example, there is a general activation sequence for menu items.</P><P>The second kind of activation report has the same form, but is specializedfor specific kinds of activation.  It is action-centric rather thancomponent-centric.  Hence, while there is a generic activation report for menuitems, there are specific report sequences for menu items that are common to allapplications.  An example of such a menu item is that used to create newdocuments, namely the &quot;New...&quot; item in the &quot;File&quot; menu.  Theaudio look and feel defines an activation report specifically for the action ofsuch document creation.  This second kind of activation report is termed the &quot;canonicalactivation report&quot;.</P><H4>General Activation Report</H4><DL><DT></DT><DT>1.  Activation sound effect.</DT><DD>A sound effect to indicate that the component has been activated.  Thissound effect is component specific.</DD><DT>2.  Name of the component.</DT><DD>The name, or textual label, of the component is spoken.</DD><DT>3.  Role of the component.</DT><DD>The type of the component is spoken.</DD><DT>4.  New state of the component.</DT><DD>The new &quot;primary&quot; state of the component is spoken.</DD></DL><P>Here are the general activation reports for <CODE>JCheckBox</CODE>,<CODE>JMenuItem</CODE>, and <CODE>JList</CODE>.</P><H5>JCheckBox</H5><OL><LI>Activation sound effect<UL><LI>Three tones.  The first and last are the same pitch.  The middle is lowerif the check box is checked, higher if unchecked.  The timbre of the tonesindicates that it is a check box.</LI></UL></LI><LI>Name of the check box.</LI><LI>Speech to indicate that is a check box.</LI><LI>Speech to indicate whether it is checked or not -- its new state.</LI></OL><H5>JMenuItem</H5><OL><LI>Menu item activation sound effect.</LI><LI>Name of the menu item.</LI><LI>No role speech.  As with the &quot;navigation-to&quot; report, it isassumed that the user knows they are activating a menu item.</LI><LI>Speech to indicate that the menu item was selected.</LI></OL><H5>JList</H5><P>The manipulation of a <CODE>JList</CODE> involves altering its selection.As noted above with respect to navigation, the audio look and feel maintains itown notion of the current item in a <CODE>JList</CODE>.  This tracking is donewithout altering the list's selection set.  The &quot;activation&quot; of thelist provides users with the ability to add or remove the current item from theselection set.  The activation report provides feedback with respect to thismanipulation:</P><OL><LI>Stapler sound effect, if adding; scissors if removing.</LI><LI>Name of item added/removed.</LI><LI>Speech to indicate that it is a list item.</LI><LI>Speech to indicate whether it was added or removed from the selection set.</LI></OL><H4>Canonical Activation Report</H4><P>The canonical activation report has the same form as the general activationreport.  The audio look and feel makes special provision to capture functions ortasks that are common to many applications.  Examples of such functions include&quot;new&quot;, &quot;open&quot;, &quot;save&quot;, &quot;quit&quot;, &quot;cut&quot;,&quot;copy&quot;, and &quot;paste&quot;.</P><P>Note that such functions are not tied to any specific user interfaceelement.  For example, although the &quot;Cut&quot; action is typically found inthe &quot;Edit&quot; menu, it is also frequently associated with a tool barbutton.  Likewise, the cut action's canonical activation report is, strictlyspeaking, not tied to any specific component.  Instead, there is a separate &quot;canon&quot;of activation reports.  When a report is required by the activation of acomponent, it first consults the canon, and if a match is found the appropriatecanonical activation report is generated.  If no canonical report is to be had,it is the component's responsibility to provide an appropriate, more general,activation report.</P><P>As an example, here is the canonical activation report for &quot;cut&quot;. Note that the activation sound effect is specific to the cutting action.  In ageneral activation report, the sound effect would be that associated withactivating the component -- a button sounds pressed, or a menu item soundsselected.  Also, the spoken name is the name of the action, not the label of thecomponent (although, it is likely that the two will be the same).</P><H5>Cut Action</H5><OL><LI>Scissors sound effect.</LI><LI>Spoken name of action, namely &quot;cutting&quot;.</LI><LI>No role speech.</LI><LI>No state speech.</LI></OL><H2>Event Driven</H2><P>Such is the structure and type of auditory feedback generated by the <CODE>ReportGenerator</CODE>class.  By itself, nothing would be &quot;displayed&quot; in the auditorymodality;  something more is needed to cause the <CODE>ReportGenerator</CODE>to generate a report. The audio look and feel accomplishes this by having the<CODE>ReportGenerator</CODE> listen for specific kinds of events, and use thoseevents to cue the relevant feedback sequence.</P><P>For example, users typically navigate to a component by moving keyboardfocus to that component.  When the component receives focus, it emits a &quot;focus-gained&quot;event.  By listening for this event, the <CODE>ReportGenerator</CODE> can cue a&quot;navigation-to&quot; report.</P><P>Which event to listen for depends on the component itself, the kinds ofevents for which it is a source, and the conditions under which it emits thoseevents.  For example, there is no guarantee that a Swing component will emit a<CODE>FocusEvent</CODE> when, from the users' point of view, they have achievedkeyboard focus.  Menu items are such components -- they are not <CODE>FocusEvent</CODE>sources.  In Swing, to navigate among menu items, keyboard focus is maintainedon the parent menu.  As one moves among its menu items, the item emits a <CODE>ChangeEvent</CODE>(to be precise, it is the menu item's model that notifies its listeners of thechange).  Thus, to cue &quot;navigation-to&quot; reports among menu items, themenu item audio user interface listens for these <CODE>ChangeEvent</CODE>'s.</P><P>It is somewhat of an art to configure a component's <CODE>ReportGenerator</CODE>in the appropriate manner.  The component, and sometimes its model, needs to bestudied to determine the events it notifies listeners of, and under whatconditions it does so notify.</P><P>To give the reader a feel for this process, consider some of the examplesequences presented above. A <CODE>JCheckBox</CODE>'s &quot;navigation-to&quot;report is cued via a &quot;focus gained&quot; event.  It's activation report,namely whether is was just checked or unchecked, is cued by an <CODE>ActionEvent</CODE>.</P><P> A <CODE>JList</CODE> that has just received focus emits a &quot;focusgained&quot; event, cueing the first version of its &quot;navigation-to&quot;sequence.  The navigation of the list's items is cued by either listening for<CODE>ListSelectionEvent</CODE>'s, or by a registered keystroke/action pair. The latter is a case where the audio user interface has installed a specific keystroke as a way of navigating the list and also forcing audio feedback.</P><P>In terms of maniuplation or activation reporting, when the user adds orremoves list items from the <CODE>JList</CODE>'s selection set, that results inthe <CODE>JList</CODE> notifying its listeners of a <CODE>ListSelectionEvent</CODE>.</P><P>Once the appropriate event set has been identified, the audio user interfaceis implemented in the following way.  Two classes are defined:  the <CODE>AudioXxxUI</CODE>,and the <CODE>AudioXxxListener</CODE>.  The &quot;Xxx&quot; stands for thecomponent in question; for example, the two classes for a <CODE>JCheckBox</CODE>are <CODE>AudioCheckBoxUI</CODE>, and <CODE>AudioCheckBoxListener</CODE>.</P><P>The <CODE>AudioXxxUI</CODE> extends the <CODE>com.sun.java.swing.plaf.XxxUI</CODE>object, and implements the requisite methods.  In particular, since this is anaudio look and feel, it defines no-op's for the <CODE>paint()</CODE> and<CODE>update()</CODE> methods.  It also instantiates an <CODE>AudioXxxListener</CODE>object and installs it as a listener on the component for whom the UI isintended.  During the UI's uninstall, the <CODE>AudioXxxUI</CODE> removes the<CODE>AudioXxxListener</CODE> from the component.</P><P>The <CODE>AudioXxxListener</CODE> extends <CODE>ReportGenerator</CODE>,and implements the identified set of event listeners.  The listener methods area means of routing control to the appropriate report generation method, whichconstructs and returns an audio sequence.  Upon receiving the sequence, thelistener methods relays it to the audio interface for its presentation.</P><P>To summarize, the audio look and feel implements a device that generatessequences of speech and non-speech audio.  This device is sensitive to theevents that the various Swing component emit.  An instance of this device isattached to an instance of a Swing component when its UI is installed; and, asthat component changes state and broadcasts those changes to its listeners, thedevice generates an appropriate audio report.</P><HR><P><STRONG>Note1:</STRONG>  need to put the fact that the audio (sound effectsand speeches) are accessed symbolically through a &quot;.properties&quot; file. Thus, the actual audio that is played is separate from the code, and is &quot;pluggable&quot;or &quot;configurable&quot;.  Part of the &quot;Audio Interface&quot; section?</P><P><STRONG>Note2:</STRONG> need to say something about the &quot;browsingwithin a context&quot;, or &quot;fast search&quot; user interface technique forcomponents that have a set of subcomponents (menus, lists, tabbed panes, etc.). Where to put?</P><HR><H2>Text</H2><P>The preceding describes how the audio look and feel generates reports formost Swing components.  However, there is a text package within Swing that hasfeatures over and above other components.  For this reason, the architecture ofthe audio look and feel for text is somewhat more involved.  This sectiondescribes the extensions of the report generating system as it appliesspecifically to text.</P><P>The text package does not quite fit the navigation/activation model usedfor other components. Navigation to a text component is basically the same,but once a text component has focus, the idea of&nbsp; 'activating' itbecomes inappropriate.<P>Once a text component has focus, the user can invoke any one of a numberof text actions. These are actions which modify the text in the component,such as inserting or deleting text, or applying attributes such as bold,or italic. Swing provides these actions through a number of editor kits,collections of text actions which can be used by text components. Typically,an application will map these actions to buttons, menu items, or keystrokes.For example, the apply-bold-attribute action may be mapped to a buttonin a toolbar, or to the keystroke <TT>Ctrl+B</TT>.<P>The Audio Look and Feel follows this editor kit model. Each of the editorkits defined by Swing has an audio equivalent, which defines an AudioTextActioncorresponding to a particular TextAction.<P>To provide audio feedback for text actions, the Audio Look and Feeluses a second report generator: the <TT>TextActionReportGenerator</TT>,which is an interface implemented by the <TT>AudioTextAction</TT>, thebase class for all audio text actions.<P>The Audio Look and Feel provides keyboard access to all of the textactions (applications will not necessarily implement keystrokes for allactions). However, only one action can be mapped to a given keystroke.Because of this, the audio action designed to provide audio feedback fora given text action cannot be mapped to the appropriate keystroke withoutoverriding the given text action. To deal with this, the audio actionsare designed to invoke their original action in order to carry out theactual action; then the audio action provides appropriate audio feedback.<H2>Text Report Generator</H2>The <TT>TextReportGenerator</TT> extends <TT>ReportGenerator</TT>, andprovides navigation reporting specific to text components.<H3>Navigation-to Report</H3>The "navigation-to" feedback sequence occurs after the user has entereda key stroke to move to a new text component. The purpose of the sequenceis to confirm the move and to describe where it is the user has "landed".<OL><LI>Navigation sound effect.</LI><BR>A sound effect to indicate that navigation has been accomplished.<LI>Role sound effect.</LI><BR>A sound effect to indicate the type of component. This sound effectis consistant across text components, indicating that it is an editablecomponent.<LI>Type of component</LI><BR>A speech to describe the component's type, or role. For example, thetype of a password field is "password field". If none can be found, thespeech is "unknown component".<LI>Name of component.</LI><BR>A speech that gives the name or textual label of the component. Ifnone can be found, the speech defaults to "no label".<LI>Current sentence</LI><BR>A speech of the current sentence in the text component.<LI>State speech</LI><BR>A speech to describe the component's current state. This is either"Ready" or "Not Available".</OL>&nbsp;Here is a concrete example, for <TT>JEditorPane</TT>.<H4>JEditorPane</H4><OL><LI>Navigation-to sound effect.</LI><LI>Editable component sound effect.</LI><BR>The sound of pencil on paper.<LI>Speech to indicate that this is an editor pane.</LI><LI>Name of the editor pane.</LI><LI>Current sentence.</LI><LI>Speech to indicate whether the editor pane is ready or not.</LI></OL><H2>Text Action Report Generator</H2>The text action report generator provides reporting of text actions. Inthe Audio Look and Feel, there are two classes of text actions: those thatare provided by existing Swing editor kits, and those specific to the AudioLook and Feel.<P>Existing Swing editor kit actions are typically actions which modifytext, such as inserting new text, or applying attributes to existing text.The <TT>TextActionReportGenerator</TT> generates audio feedback for theseactions.<P>Text actions specific to the Audio Look and Feel are akin to 'where-am-I'actions, but pertain to where the user is <I>within the contents</I> ofthe text component. They are typically requests for the Audio Look andFeel to speak portions of the text, such as the current word, or the previoussentence. The <TT>TextActionReportGenerator</TT> is used to generate thisspeech.<H3>TextActionReport</H3>All elements of the text action report are optional, based on the actionitself. In many cases, only one element may be present.<DL><OL><LI>Action sound effect.</LI><BR>A sound effect to indicate the action taken.<LI>Action speech.</LI><BR>A speech that identifies the action taken<LI>Action state speech.</LI><BR>A speech to describe the state resulting from the action.<LI>Relevant text.</LI><BR>The text affected by the action.</OL><P>Here are three concrete examples, one for <TT>backspace</TT>, one for<TT>bold</TT>, and another for <TT>speak-current-sentence</TT>.<H4>backspace</H4><OL><LI>Backspace sound effect.</LI><LI>No action speech.</LI><LI>No state speech.</LI><LI>No relevant text.</LI></OL><H4>bold</H4><OL><LI>Toggle-on sound effect.</LI><LI>Speech to indicate the action, in this case, bold.</LI><LI>Speech to indicate the state of the attibute, in this case, on.</LI><LI>Speech of the text to which the bold attribute was applied.</LI></OL><H4>speak-current-sentence</H4><OL><LI>No action sound effect</LI><LI>No action speech.</LI><LI>No state speech.</LI><LI>Speech of the current sentence.</LI></OL></DL><H2>Audio Component</H2><P>Swing components generate audio feedback as described above.  Given such a feedback sequence, how is it actually played?  This next section describes the audio interface aspect of the system.</P><p>The audio component is a term applied to those portions of thesystem which produce the sounds and speech used by the audio lookand feel. The audio component is multi-threaded and consists ofthe following major classes:<dl>    <dt>atrcuof.plaf.audio.Report <dd>a collection of sounds and spoken    words or phrases that will be heard by the end user.    <dt>atrcuof.plaf.audio.ReportProcessor <dd>a class which    implements the Runnable Interface and is responsible for    presenting a Report to the end user.    <dt>atrcuof.plaf.audio.AudioInterface <dd>a class which provides an    interface between the "low-level" classes that produce    sounds and speech and the "look and feel" portions of the    system.    <dt>atrcuof.plaf.audio.AudioInterface$RPManager <dd>an inner class    of AudioInterface responsible for stopping, starting and    monitoring ReportProcessors.    <dt>atrcuof.plaf.audio.PlafSynthesizer <dd>the low-level class    responsible for starting, stopping, and monitoring speech.    This class makes extensive use of IBM's Synthesizer class in    their implementation of the Java Speech API.    <dt>atrcuof.plaf.audio.JMFPlayable <dd>the class low-level class    responsible for starting, stopping, and monitoring sound    production. This class makes extensive use of Sun's Java    Media Foundation Player class.</dl> <h3>Threads in the Audio Component</h3><p>The audio component makes use of a number of Threads, bothlocal and non-local to itself:<dl>    <dt>AWT Event Handler <dd>standard AWT processing; Report building    and the queueing of a Report for execution probably takes    place in this thread.    <dt>ReportProcessor thread <dd>one for each ReportProcessor. Starts    Players and Synthesizers. Goes into wait state after a sound    or speakable has been started. Awakened via notify() by    completion code from Player or JSAPI.    <dt>Event thread for Players and event thread for Synthesizers.    <dd>Gets control when events associated with these objects    occur. Will notify() the wait()ing ReportProcessor when    the sound or speech event is complete.    <dt>AudioInterface$RPManager Event Handler <dd>co-ordinates    multiple Reports by starting and stopping ReportProcessors.</dl><h3>The Report Class</h3><p>A Report is a collection of instances of class SoundThing.A SoundThing represents either a file to be played or a pieceof text to be spoken and includes methods to communicate with thelower-level objects which actually handle the production of soundor speech. Associated with each SoundThing is a time to waitbefore producing the next sound/speech and an optional "voice"to use when speaking.<p>The Report class makes extensive use of the plaf propertiesfile to enable a "level of indirection" in the creation of aReport. When a Report is created by the ReportGeneratorcomponent, the Report's constructor can utilize symbols fromthe properties file to represent sound files or strings to bespoken. These symbols will be replaced by the symbols' valueswhen the constructor is executed. Since the properties file iseditable, the end user will be able to tailor the look and feelto suit her own preferences, perhaps choosing different sounds(or none at all) or changing the words or phrases that theprogram will speak.<p>There are two types of Reports: Reports which wait until allpreviously submitted Reports have completed and Reports whichinterrupt any currently running Reports. The ReportGeneratorcomponent specifies the type of Report at construction time andqueues the Report for execution by invoking the doReport methodof atrcuof.plaf.audio.AudioInterface.<h3>The ReportProcessor Class</h3><p>Each Report is assigned to an instance of class ReportProcessorwhich is responsible for starting, stopping, and monitoring theReport's execution. By having a seperate ReportProcessor assignedto each Report, as yet un-implemented features such as pausingand resuming Reports and continuous playing of a sound file canbe easily performed.<p>Each ReportProcessor has a Thread associated with it. Thesynchronized run method begins with a wait(). ClassAudioInterface$RPManager assigns a Report to a ReportProcessorand then issues a notify() call to awaken the ReportProcessorwhich then enters a loop in which each SoundThing in the Reporthas its playSound() method invoked. This method in turn invokesthe lower-level objects to start the sound file playing or beginspeaking the word or phrase. Since both speech and soundcomplete asynchronously, the ReportProcessor issues anotherwait() call and sleeps until it is awakened by the low-levelobject upon completion of the sound or speech. After all theSoundThings in the Report have completed their work, theReportProcessor again enters its top-of-loop wait().<p>Another significant method in ReportProcessor iscancelRunningReport. This synchronized method is invoked byAudioInterface$RPManager when an interrupt Report has beensubmitted for processing. cancelRunningReport sets a switch totell run() not to start any more SoundThings once the current onecompletes and then invokes the stopSound() method of the currentlyrunning SoundThing.<h3>The AudioInterface$RPManager Class</h3><p>AudioInterface$RPManager is an inner class of AudioInterface thatis responsible for managing the ReportProcessors. Othercomponents communicate with it by placing requests on a queue andissuing a notify() call to AudioInterface$RPManager's eventhandler thread to tell it that a request is available forprocessing.<p>AudioInterface$RPManager performs its functions byoperating on a set of queues. These queues are:<dl>    <dt>eventQueue <dd>the queue from which the event handler thread    gets its requests.    <dt>availableProcessors <dd>a queue holding ReportProcessors that    are available for work (i.e. they have no Reports assigned    to them).    <dt>runnableProcessors <dd>a queue holding ReportProcessors that    are ready to run. Typically these ReportProcessors are ready    to process a "wait" Report, but can't start until an    existing Report has finished.    <dt>runningProcessors <dd>a queue on which the ReportProcessor    with the currently running Report resides.    <dt>transitionProcessors <dd>a queue for ReportProcessors whose    work has been terminated by an invocation of the    cancelRunningReport method.</dl><p>The AudioInterface$RPManager's event handler thread operates in aloop in which it wait()s for notification that a request isavailable for processing. Upon awakening, it removes the firstrequest from the queue and processes it. It continuesprocessing requests until the queue is empty whereupon it againwait()s for request notification.<p>The following requests are processed by theAudioInterface$RPManager's event handler thread:<dl>    <dt>RPMEvent.RPME_NEW_INT_REPORT <dd>a new Interrupt Report has    been submitted for processing. This event is processed as    follows:        <p>Each ReportProcessor on RunningProcessors queue is        removed from the queue and its cancelRunningReport method        is invoked. The ReportProcessor is then moved to the        transitionProcessors queue. An available ReportProcessor        is acquired from the availableProcessors queue and the        Report is assigned to it. This ReportProcessor is placed        on the runningProcessors queue and is then started.        Finally, any ReportProcessors on the runnableProcessors        queue are removed from that queue and added to the        availableProcessors queue.    <dt>RPMEvent.RPME_NEW_WAIT_REPORT <dd>a new Wait Report has been    submitted for processing. This event is processed as follows:        <p>An available ReportProcessor is acquired from the        availableProcessors queue and given the Report. The        ReportProcessor is then placed the runnableProcessors        queue. If no ReportProcessors are running, a        ReportProcessor is moved from the runnableProcessors        queue to the runningProcessors queue and started.    <dt>RPMEvent.RPME_RP_COMPLETED <dd>A ReportProcessor has finished    processing a Report. This event can happen in two ways: the    ReportProcessor could have completed normally or it could    have been terminated prematurely when its    cancelRunningReport method was invoked. In either case the    ReportProcessor, as part of its windup processing, will have    constructed a RPME_RP_COMPLETED request and placed it on    AudioInterface$RPManager's request queue. This event is    processed as follows:        <p>The ReportProcessor is moved from the        transitionProcessors queue to the availableProcessors        queue. If no ReportProcessors are running and the        runnableProcessors queue is not empty, a ReportProcessor        is moved from that queue to the runningProcessors queue        and started.    <dt>RPMEvent.RPME_STOP_AUDIO <dd>the AudioInterface's stopAllAudio    method has been invoked. This event is processed as    described above for the submission of an interrupt Report,    but no new Reports are started.</dl><h3>The AudioInterface Class</h3><p>The AudioInterface class has two primary responsibilities: actingas an interface between the audio component and the rest ofthe audio look and feel; and managing the low-level audioclass instances which are responsible for the actual production ofsound and speech.<p>The interface function of AudioInterface consists of severalmethods:<dl>    <dt>initialize <dd>creates the JSAPI synthesizer(s) and enables the    output of sound. Prior to the invocation of this method,    any created Report are discarded.    <dt>doReport <dd>this method receives a Report object from the    ReportGenerator component, queries the report as to its    type (cancel or wait) and creates either a RPMEvent.    RPME_NEW_WAIT_REPORT event or a RPMEvent.    RPME_NEW_INT_REPORT event for submission to the    AudioInterface$RPManager. Submission is as described above.    <dt>stopAllAudio <dd>this method causes the creation and submission    of a RPMEvent.RPME_STOP_AUDIO event which is processed as    described above.</dl><p>The low-level object management functions of AudioInterface areaccessed by SoundThing instances when their playSound method isinvoked by a ReportProcessor.<p>For a Speakable SoundThing, the AudioInterface will supply areference to Synthesizer object of the appropriate voice.<p>For a JMFPlayable, AudioInterface manages a hash table keyed onthe name of the sound file to be played. The first time a file isreferenced by a Report (during its constructor processing), aJMF Player object is created and brought to a state of readinessfor playing the sound. Subsequent references to the hash tablereturn the previously created Player and permit a significantlyfaster startup of the sound.<HR><P>Copyright (C) 1998, 1999 Adaptive Technology Resource Centre, University ofToronto.</P><P>Verbatim copying and distribution of this entire article is permitted in anymedium, provided this notice is preserved.</P><P>Updated: 1999 Sep 08 JS</P><P>Web site maintained by<A HREF="mailto:clown.scheuhammer@utoronto.ca">Joseph Scheuhammer</A></P></BODY></HTML>